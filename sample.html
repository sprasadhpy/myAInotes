<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Beyond Preference in AI Alignment</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      background-color: #f0f8ff;
      color: #333;
      line-height: 1.6;
      margin: 0;
      padding: 0;
    }
    header {
      background-color: #4a90e2;
      color: white;
      padding: 20px;
      text-align: center;
    }
    h1 {
      margin: 0;
    }
    .content {
      padding: 20px;
      max-width: 800px;
      margin: 40px auto;
      background-color: #fff;
      border-radius: 8px;
      box-shadow: 0 0 10px rgba(0,0,0,0.1);
    }
    h2 {
      color: #4a90e2;
    }
    ul {
      list-style-type: square;
      margin-left: 20px;
    }
    footer {
      text-align: center;
      padding: 20px;
      background-color: #4a90e2;
      color: white;
      position: fixed;
      bottom: 0;
      width: 100%;
    }
  </style>
</head>
<body>

<header>
  <h1>Beyond Preference in AI Alignment</h1>
</header>

<div class="content">
  <h2>Problems</h2>
  <ul>
    <li>Current AI alignment approaches focus too much on human preferences and fail to capture the complexity of human values and decision-making.</li>
    <li>Rational choice and utility theories assume that preferences can be fully quantified, but they do not account for incommensurable values, i.e., values that can't be easily compared or measured or complex decision-making processes.</li>
    <li>Aggregating preferences from multiple individuals is challenging, both computationally and politically. It’s difficult to combine conflicting preferences into a coherent whole.</li>
    <li>There’s confusion between different kinds of preferences: self-regarding preferences, all-things-considered preferences, and elicited preferences, which makes aggregation more complicated.</li>
    <li>Majority-based aggregation can lead to harmful biases, especially when sensitive issues like discrimination or minority concerns are not well-represented, creating epistemic injustice.</li>
    <li>The vision of a single AI system optimizing humanity’s collective preferences is not realistic due to computational complexity, political impracticality, and the risk of concentrating power in a few hands.</li>
  </ul>

  <h2>Solutions Suggested by the Authors</h2>
  <ul>
    <li>AI should be aligned with “collectively negotiated norms” and principles (social, legal, and moral) that respect the complexity of human values.</li>
    <li>Adopt a contractualist model where AI systems are aligned with norms that different stakeholders can agree upon.</li>
    <li>Embrace a pluralistic approach, where multiple AI systems are aligned with different norms tailored to specific contexts and roles.</li>
    <li>Ensure that AI alignment considers incentives of different actors (developers, stakeholders) and aligns AI to promote cooperation and minimize conflict.</li>
    <li>Focus on aligning AI with high-level norms rather than trying to optimize preferences to reduce computational complexity and improve political agreement.</li>
  </ul>

  <p>In summary, limitations of preferentist approaches to AI alignment can be solved by shifting toward norm-based, contractualist, and pluralistic frameworks that account for the diversity of human values and political realities.</p>
</div>

<footer>
  <p>&copy; 2024 AI Alignment Blog</p>
</footer>

</body>
</html>
