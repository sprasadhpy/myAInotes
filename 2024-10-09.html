<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Beyond Preference in AI Alignment</title>
  <link rel="stylesheet" href="/assets/css/style.css"> <!-- Link your CSS here -->
  
  <style>
    /* Inline styles if you don't want a separate CSS file */
    body {
      font-family: 'Arial', sans-serif;
      line-height: 1.8;
      color: #333;
      background-color: #f9f9f9;
      margin: 0;
      padding: 0;
    }
    header {
      background-color: #4a90e2;
      color: white;
      padding: 20px;
      text-align: center;
    }
    h1 {
      font-size: 2.5em;
      margin: 0;
    }
    .content {
      max-width: 800px;
      margin: 40px auto;
      padding: 20px;
      background-color: white;
      box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);
      border-radius: 8px;
    }
    h2 {
      color: #4a90e2;
    }
    ul {
      list-style-type: disc;
      margin-left: 20px;
    }
    li {
      margin-bottom: 10px;
    }
    footer {
      text-align: center;
      padding: 10px; /* Reduced padding size */
      background-color: #4a90e2;
      color: white;
      position: fixed;
      bottom: 0;
      width: 100%;
      font-size: 0.85em; /* Reduced font size */
    }
    .paper-source {
      margin-top: 20px;
      text-align: center;
      font-size: 0.9em;
    }
  </style>
</head>
<body>

<header>
  <h1>Beyond Preference in AI Alignment</h1>
</header>

<div class="content">
  <h2>Problems</h2>
  <ul>
    <li>Current AI alignment approaches focus too much on human preferences and fail to capture the complexity of human values and decision-making.</li>
    <li>Rational choice and utility theories assume that preferences can be fully quantified, but they do not account for incommensurable values, i.e., values that can't be easily compared or measured or complex decision-making processes.</li>
    <li>Aggregating preferences from multiple individuals is challenging, both computationally and politically. It’s difficult to combine conflicting preferences into a coherent whole.</li>
    <li>There’s confusion between different kinds of preferences: self-regarding preferences (individual interests), all-things-considered preferences (moral and social values), and elicited preferences (from AI alignment processes), and these differences make aggregation more complicated.</li>
    <li>Majority-based aggregation can lead to harmful biases, especially when sensitive issues like discrimination or minority concerns are not well-represented, creating epistemic injustice.</li>
    <li>The vision of a single AI system optimizing humanity’s collective preferences is not realistic due to several factors such as computational complexity, political impracticality, and the risk of concentrating power in a few hands.</li>
  </ul>

  <h2>Solutions Suggested by the Authors</h2>
  <ul>
    <li>Rather than aligning AI systems with individual preferences, AI should be aligned with “collectively negotiated norms” and principles (social, legal, and moral) that respect the complexity of human values. <strong>– Norm-Based AI Alignment</strong></li>
    <li>Adopt a <strong>contractualist model</strong> for alignment where AI systems are aligned with norms that different stakeholders can agree upon. This respects individuality and is more feasible politically and practically.</li>
    <li>It's important to embrace a <strong>pluralistic approach</strong>, where multiple AI systems are aligned with different norms tailored to specific contexts and roles. This avoids the problem of one-size-fits-all preference aggregation.</li>
    <li>Ensure that AI alignment considers the most important aspect—<strong>incentives of different actors</strong> (developers, stakeholders)—and aligns AI in a way that promotes cooperation and minimizes conflict.</li>
    <li>Focus on aligning AI with <strong>high-level norms</strong> rather than trying to optimize preferences. This is more practical because it reduces the computational complexity and makes political agreement more achievable.</li>
  </ul>

  <p>To summarize, limitations of <strong>preferentist approaches</strong> to AI alignment can be solved by a shift toward <strong>norm-based, contractualist, and pluralistic frameworks</strong> that better account for the diversity of human values and political realities.</p>

  <div class="paper-source">
    <p>For more details, see the original paper: <a href="https://arxiv.org/pdf/2408.16984" target="_blank">Beyond Preference in AI Alignment (arXiv: 2408.16984)</a></p>
  </div>
</div>

<footer>
  <p>&copy; (c) 2024 my AI notes</p>
</footer>

</body>
</html>
